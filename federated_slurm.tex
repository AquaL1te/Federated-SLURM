\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{footnote}
\usepackage{multicol}
\usepackage{float}
\usepackage{minted}
\usepackage{todonotes}
\usepackage{url}[hyphens]
\usepackage[acronym, nonumberlist]{glossaries}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Preliminary evaluation of federated SLURM}

\author{\IEEEauthorblockN{Kees de Jong, Maxim Masterov}
\IEEEauthorblockA{\textit{SURFsara} \\
Amsterdam, The Netherlands \\
kees.dejong@surfsara.nl, maxim.masterov@surfsara.nl}}

\maketitle

\begin{abstract}
\gls{cca} has as a goal to consolidate functionality on a common hardware infrastructure. The hypothesis is that this will create a more cost-effective system, easier management and more flexible access to \gls{hpc} and high performance storage resources. In practice this concept is planned for the integration of Cartesius \cite{cartesius-userinfo} and Lisa \cite{lisa-userinfo} hardware. This research investigated the advantages and disadvantages of a federated \gls{slurm} as a first step of a more unified integration of Cartesius and Lisa.
\end{abstract}

\begin{IEEEkeywords}
Federated SLURM, HPC, unified computing
\end{IEEEkeywords}


\section{Introduction}
\subsection{Background}
\label{sec-background}
\gls{slurm} provides the means to allocate exclusive and/or non-exclusive access to typically \gls{hpc} compute resources for a duration of time \cite{wiki-slurm}. Therefore, \gls{slurm} provides a scheduling framework for starting, executing, and monitoring compute jobs. These are typically parallel \gls{mpi} jobs on a set of scheduled compute nodes. \gls{slurm} also provides the intelligence to manage queues and thus congestion of the compute resources.

Support for creating a federation of clusters was included with the release of \gls{slurm} 17.11. This feature enables users to schedule jobs on a range of clusters with a unified and unique job ID among all clusters \cite{slurm-federated-guide}. Each cluster then independently attempts to schedule the job according to its own scheduling policies. Furthermore, the cluster where the job was submitted to (origin cluster) coordinates with the other clusters in the federation to schedule the job. This is done by submitting copies of the job (sibling jobs) to each eligible cluster.


\subsection{Experimental setup}
SURFsara intends to integrate the Cartesius and Lisa systems on e.g. the job scheduling level. Federated \gls{slurm} could assist in this ambition. In order to experiment with federated \gls{slurm}, an experimental setup was used (table \ref{tab-experimental-setup}). It is assumed that a true \gls{slurm} federation will not share a uniform setup. Thus, our experimental setup reflects this by using different versions of \gls{slurm} and Linux distributions.

\begin{table}[H]
\begin{center}
\caption{Experimental setup}
\label{tab-experimental-setup}
\begin{tabular}{llll}
\textbf{Cluster name} & \textbf{Linux distribution} & \textbf{SLURM} & \textbf{MariaDB} \\
fedora                & Fedora 31                   & 19.05.4        & 10.3.20          \\
debian                & Debian 10                   & 18.08.5        & 10.3.18          \\
ubuntu                & Ubuntu 18.04.3 LTS          & 17.11.2        & 10.1.29         
\end{tabular}
\end{center}
\end{table}


\subsection{Known limitations}
\label{sec-limitations}
SchedMD (current maintainer of \gls{slurm}) highlighted the following known limitations of a federated cluster \cite{slurm-federated-guide}.

\begin{itemize}
    \item A federated job that fails due to resources (partition, node counts, etc.) on the local cluster will be rejected and will not be submitted to other sibling clusters even if it could run on them.
    \item Job arrays only run on the cluster that they were submitted to.
    \item Job dependencies are not supported across clusters.
    \item Job modification must succeed on the origin cluster for the changes to be pushed to the sibling jobs on remote clusters.
    \item Modifications to anything other than jobs are disabled in \mintinline{bash}{sview}.
    \item \mintinline{bash}{sview} grid is disabled in a federated view.
    \item Limited size of federation (64 clusters).
    \item JodIDs are stored in 26 bits, not in 32, as in non-federated \gls{slurm}.
    \item A cluster can be a part of only one federation at a time.
    \item Jobs can't span multiple clusters.
    \item A \gls{slurm} federation is not intended as a high-throughput environment (\textgreater 50,000 jobs a day). However, this can be mediated by using \mintinline{bash}{--cluster-constraint} or the \mintinline{bash}{-M} submission options to limit the amount of clusters the sibling jobs can be submitted to or directing load to.
\end{itemize}


\subsection{Research question}
This section briefly discusses the research questions based on the use case and experimental setup (section \ref{sec-background}), and the known limitations (section \ref{sec-limitations}). The term federated \gls{slurm} implies central control over independent clusters. The main research question therefore is; how federated can federated \gls{slurm} actually be and what are the advantages and disadvantages? Furthermore, how applicable is federated \gls{slurm} for the Cartesius and Lisa systems?


\section{Results}
\subsection{Cluster setup}
When using \gls{slurm}, a central database may used to archive job accounting. The daemon used for this is called \mintinline{bash}{slurmdbd}. The \gls{slurm} controller (\mintinline{bash}{slurmctld}) is responsible for monitoring all the \gls{slurm} daemons and resources, accepts work (jobs), and allocates resources to those jobs \cite{slurm-slurmctld}. The \mintinline{bash}{slurmctld} communicates this information to the \mintinline{bash}{slurmdbd} to be registered to a supported database, i.e. MariaDB \cite{mariadb}. The \mintinline{bash}{slurmdbd} must be at the same or higher major release number as the \mintinline{bash}{slurmctld} daemon \cite{slurm-upgrade-guide}. Therefore, the only valid candidate to be the \mintinline{bash}{slurmdbd} in our experimental setup (table \ref{tab-experimental-setup}) is the Fedora 31 system, due to having the highest \gls{slurm} release number.

Furthermore, \gls{slurm} permits upgrades to a new major release from the past two major releases without loss of jobs or other state information, which happen every nine months. State information and RPCs from older versions will not be recognized and will be discarded, resulting in loss of all running and pending jobs. Therefore, the \mintinline{bash}{slurmdbd} version number can only go up. When other \gls{slurm} clusters run a lower release number of the \mintinline{bash}{slurmdbd}, then this results in compatibility problems when this \gls{slurm} cluster decides to leave the federation. Therefore, the best compatibility is ensured by having the \gls{slurm} daemons in all clusters (regardless of which Linux distribution is used), running the same major version release number.

There must a uniform user (UID) and group (GID) name space accros a \gls{slurm} cluster. This requirement is twofold; to ensure storage permission compatibility throughout the distributed cluster (and network attached file systems) and to ensure that the \mintinline{bash}{SlurmUser} option in \mintinline{bash}{slurm.conf} is resolved to the same user. Therefore, all federated \gls{slurm} clusters must use a common user administration database, such as LDAP.

In order to authenticate the UID and GID of another local or remote process (RPC), an authentication mechanism is used called \mintinline{bash}{munge} \cite{github-munge}. The key used for authentication must be distributed throughout the \gls{slurm} cluster. However, it is possible to run multiple \mintinline{bash}{munge} keys by running multiple daemon instances pointing to different keys. Another authentication requirement is time (to prevent replay attacks), thus clocks need to be synchronized throughout the \gls{slurm} cluster, this is usually done by NTP.




The following federated \gls{slurm} options can be used when submitting a job \todo{the listing below could use some sources}.

\begin{itemize}
    \item Clusters can be assigned with a weight which can be used to influence job allocation.
    \item The job can be submitted onto busiest cluster to reduce fragmentation.
    \item The job can be submitted onto least loaded cluster (LLC flag).
    \item The job can be submitted onto particular cluster using -M flag.
\end{itemize}

% coherent jobs? mpi versions, lib versions

% \gls{slurm} requires a database for job accounting. Therefore, the question is; does every federated cluster account for its own job activity? Or is there a central database for all?

% Different SLURM versions might require different database tables. Therefore, the questions is; how flexible can a federated cluster be with their SLURM versions?

% What is the general need to have a federated cluster? A PRACE/DEISA federated load leveler project \cite{radecki2014reservations} was not successful due to a lack of need for such a solution [TODO: Citation needed].

% When creating job scripts, some infrastructure specific variables are defined, i.e. mounted file systems, paths to certain software or data. With a federated cluster, people could launch jobs on different clusters, which could complicate the user experience.

% How to control available resources and provide users with the optimal solution (via sbatch)? A federated cluster assumes (by default) that the running code is well developed and parallelized. In reality it is far from truth. Thus, some applications will demonstrate poor performance when executed on "non-friendly" hardware.

% Based on these research questions a general research question emerges; how federated can a cluster really become? Or will it be needed to make these infrastructures uniform, which goes against the federation principle. If for example different clusters will have to adapt to a common and thus uniform design, then the concept of federation is lost.






\section{Discussion}
% discuss what was done
% discuss the findings


\section{Conclusion}
% evaluate the findings discussed in discussion with the known limitations
% conclude the pros and cons
% briefly discuss the prace unicore project and the leassons we can learn from that based on the findings of this rapport


\section{Future work}
% list the stuff that we intended to do, but didn't conclude due to time restrictions





\newacronym{slurm}{SLURM}{Simple Linux Utility for Resource Management}
\newacronym{cca}{CCA}{Central Converged Architecture}
\newacronym{hpc}{HPC}{High Performance Computing}
\newacronym{mpi}{MPI}{Message Passing Interface}


\bibliographystyle{./bibliography/IEEEtran}
\bibliography{./bibliography/IEEEabrv,./bibliography/IEEEexample}

\end{document}